{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/13793/Desktop/aas/stock\"))\n",
    "import processer as processer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13793\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:172: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kickers on my watchlist xide tit soq pnk cpw b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aap movie 55 percent return for the fea and ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id be afraid to short amzn to they are looking...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnta over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oi over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>industry body cii said are likely to suffer a ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>prices slip below rs 46000 as book profits ami...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>workers at bajaj auto have agreed to a 10 perc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>live sensex off days high up 600 points test...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>climb off days highs still up 2 percent key fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5791 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "0     kickers on my watchlist xide tit soq pnk cpw b...          1\n",
       "1     aap movie 55 percent return for the fea and ge...          1\n",
       "2     id be afraid to short amzn to they are looking...          1\n",
       "3                                       mnta over 12.00          1\n",
       "4                                         oi over 21.37          1\n",
       "...                                                 ...        ...\n",
       "5786  industry body cii said are likely to suffer a ...         -1\n",
       "5787  prices slip below rs 46000 as book profits ami...         -1\n",
       "5788  workers at bajaj auto have agreed to a 10 perc...          1\n",
       "5789  live sensex off days high up 600 points test...          1\n",
       "5790  climb off days highs still up 2 percent key fa...          1\n",
       "\n",
       "[5791 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/13793/Desktop/aas/stock/twitter.csv', encoding='ISO-8859-1')\n",
    "data = processer.Preprocess_Tweets(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification the positive/negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear up meaningless words for NB\n",
    "Stop = set([s.replace(\"'\", '') for s in stopwords.words('english') if s not in ['not', 'up', 'down', 'above', 'below', 'under', 'against','no','shouldnt']])\n",
    "data['Text'] = data['Text'].apply(lambda s: \" \".join([word for word in s.split() if word not in Stop]))\n",
    "data['Text'] = data['Text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Accuracy: 66.14 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SentimentIntensityAnalyzer()\n",
    "#baseline\n",
    "data['V_score'] = data['Text'].apply(lambda score: model.polarity_scores(score)['compound'])\n",
    "data['V_prediction'] = data['V_score'].apply(lambda score: 1 if score>=0 else -1)\n",
    "print('Vader Accuracy:', round((len(data[data['Sentiment']==data['V_prediction']])/len(data)) *100, 2), '%', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  0.6   |  Best Score:  84.35\n",
      "Naive-Bayes Accuracy: 71.7 %\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayesian Train/Test\n",
    "train_pct = .8\n",
    "np.random.seed(100)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "\n",
    "# Calculate TF-IDF for Naive Bayes classification\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "# Get TF-IDF for Train and Test data\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "X_test_tfidf = tf_idf.transform(X_test)\n",
    "\n",
    "\n",
    "# Define function to determine accuracy of model\n",
    "def get_auc_CV(model):\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    auc = cross_val_score(model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()\n",
    "\n",
    "# Find best performing model\n",
    "alphas = np.arange(0.4,1.5,0.1)\n",
    "models = [MultinomialNB(alpha=i) for i in alphas]\n",
    "accs = []\n",
    "for model in models:\n",
    "    accs.append(get_auc_CV(model))\n",
    "accs = np.array(accs)\n",
    "best_alpha = round(alphas[accs.argmax()], 1)\n",
    "\n",
    "# Print best alpha value and accuracy\n",
    "print('Best alpha: ', best_alpha, '  |  Best Score: ', round(accs.max()*100, 2))\n",
    "\n",
    "# Retrain best performing model\n",
    "best_model = MultinomialNB(alpha=best_alpha)\n",
    "best_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict test data with best model\n",
    "probs = best_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Print accuracy of best performing model on tweet sentiment analysis \n",
    "print('Naive-Bayes Accuracy:', round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes Accuracy (K-Fold): 71.804 %\n"
     ]
    }
   ],
   "source": [
    "#K-Fold\n",
    "kfold = KFold(n_splits=5,\n",
    "                 shuffle=True,\n",
    "                 random_state=132)\n",
    "mses = np.zeros((1, 5))\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train = data.loc[train_index]['Text']\n",
    "    y_train = data.loc[train_index]['Sentiment']\n",
    "    y_train[y_train==-1] = 0\n",
    "    X_test = data.loc[test_index]['Text']\n",
    "    y_test = data.loc[test_index]['Sentiment']\n",
    "    y_test[y_test==-1] = 0\n",
    "    # Calculate TF-IDF for Naive Bayes classification\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "    # Get TF-IDF for Train and Test data\n",
    "    X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "    X_test_tfidf = tf_idf.transform(X_test)\n",
    " \n",
    "    best_model = MultinomialNB(alpha=0.6)\n",
    "    best_model.fit(X_train_tfidf, y_train)\n",
    "    probs = best_model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mses[0,i] = round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2)\n",
    "    i=i+1\n",
    "print('Naive-Bayes Accuracy (K-Fold):', mses.mean(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "train_pct = .8\n",
    "np.random.seed(10)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT processing function\n",
    "def prep(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in data:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=i, \n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\", \n",
    "                return_attention_mask=True )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  48\n"
     ]
    }
   ],
   "source": [
    "#Setting up for BERT\n",
    "encoded = [tokenizer.encode(i, add_special_tokens=True) for i in data['Text'].values]\n",
    "MAX_LEN = max([len(i) for i in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "X_train_inputs, X_train_masks = prep(X_train)\n",
    "X_test_inputs, X_test_masks = prep(X_test)\n",
    "\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        input_layer = 768\n",
    "        hidden_layer = 40\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for i in self.bert.parameters():\n",
    "                i.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        layer = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(layer)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.51820  |  Test Loss: 0.45014  |  Test Accuracy: 80.22\n",
      "Epoch: 2  |  Train Loss: 0.32035  |  Test Loss: 0.42441  |  Test Accuracy: 80.49\n",
      "Epoch: 3  |  Train Loss: 0.16949  |  Test Loss: 0.56377  |  Test Accuracy: 83.00\n",
      "Epoch: 4  |  Train Loss: 0.07111  |  Test Loss: 0.77743  |  Test Accuracy: 82.74\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(freeze=False)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "lr = 8e-5\n",
    "eps = 1e-8\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data; NOTICE: this accuracy function has to align with previous models\n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13793\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7622377622377622\n",
      "0.8643835616438356\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(freeze=False)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('stock_sentiment_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_inputs, batch_masks, batch_labels = batch\n",
    "\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "        \n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "negatives = np.where(y_test==0)[0]\n",
    "TNs = np.where( (y_test==0) & (y_test==predictions) )[0]\n",
    "print(len(TNs)/len(negatives))\n",
    "\n",
    "positives = np.where(y_test==1)[0]\n",
    "TPs = np.where( (y_test==1) & (y_test==predictions) )[0]\n",
    "print(len(TPs)/len(positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
