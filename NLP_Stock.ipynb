{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Pick the best Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/13793/Desktop/aas/stock\"))\n",
    "import processer as processer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data with 'processer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13793\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:172: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kickers watchlist xide tit soq pnk cpw bpz aj ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aap movie 55 percent return fea geed indicator...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id afraid short amzn looking like near monopol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnta 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oi 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>industry body cii said likely suffer net reven...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>prices slip below rs 46000 book profits amid l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>workers bajaj auto agreed 10 percent wage cut ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>live sensex days high up 600 points tests 92...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>climb days highs still up 2 percent key factor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5791 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment\n",
       "0     kickers watchlist xide tit soq pnk cpw bpz aj ...          1\n",
       "1     aap movie 55 percent return fea geed indicator...          1\n",
       "2     id afraid short amzn looking like near monopol...          1\n",
       "3                                            mnta 12.00          1\n",
       "4                                              oi 21.37          1\n",
       "...                                                 ...        ...\n",
       "5786  industry body cii said likely suffer net reven...         -1\n",
       "5787  prices slip below rs 46000 book profits amid l...         -1\n",
       "5788  workers bajaj auto agreed 10 percent wage cut ...          1\n",
       "5789  live sensex days high up 600 points tests 92...          1\n",
       "5790  climb days highs still up 2 percent key factor...          1\n",
       "\n",
       "[5791 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trained the model by a data set with text and sentiment\n",
    "data = pd.read_csv('C:/Users/13793/Desktop/aas/stock/twitter.csv', encoding='ISO-8859-1')\n",
    "data = processer.Preprocess_Tweets(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Models: \n",
    "\n",
    "1. Base Line vader Sentiment analysis model\n",
    "\n",
    "2. Naive Bayesian model\n",
    "\n",
    "3. BERT NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Accuracy: 66.14 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentimentIntensityAnalyzer()\n",
    "#baseline\n",
    "data['V_score'] = data['Text'].apply(lambda score: model.polarity_scores(score)['compound'])\n",
    "data['V_prediction'] = data['V_score'].apply(lambda score: 1 if score>=0 else -1)\n",
    "print('Vader Accuracy:', round((len(data[data['Sentiment']==data['V_prediction']])/len(data)) *100, 2), '%', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  0.6   |  Best Score:  84.35\n",
      "Naive-Bayes Accuracy: 71.7 %\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayesian Train/Test\n",
    "train_pct = .8\n",
    "np.random.seed(100)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "\n",
    "# Calculate TF-IDF for Naive Bayes classification\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "# Get TF-IDF for Train and Test data\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "X_test_tfidf = tf_idf.transform(X_test)\n",
    "\n",
    "\n",
    "# Define function to determine accuracy of model\n",
    "def get_auc_CV(model):\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    auc = cross_val_score(model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()\n",
    "\n",
    "# Find best performing model\n",
    "alphas = np.arange(0.4,1.5,0.1)\n",
    "models = [MultinomialNB(alpha=i) for i in alphas]\n",
    "accs = []\n",
    "for model in models:\n",
    "    accs.append(get_auc_CV(model))\n",
    "accs = np.array(accs)\n",
    "best_alpha = round(alphas[accs.argmax()], 1)\n",
    "\n",
    "# Print best alpha value and accuracy\n",
    "print('Best alpha: ', best_alpha, '  |  Best Score: ', round(accs.max()*100, 2))\n",
    "\n",
    "# Retrain best performing model\n",
    "best_model = MultinomialNB(alpha=best_alpha)\n",
    "best_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict test data with best model\n",
    "probs = best_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Print accuracy of best performing model on tweet sentiment analysis \n",
    "print('Naive-Bayes Accuracy:', round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes Accuracy (K-Fold): 71.804 %\n"
     ]
    }
   ],
   "source": [
    "#K-Fold\n",
    "kfold = KFold(n_splits=5,\n",
    "                 shuffle=True,\n",
    "                 random_state=132)\n",
    "mses = np.zeros((1, 5))\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train = data.loc[train_index]['Text']\n",
    "    y_train = data.loc[train_index]['Sentiment']\n",
    "    y_train[y_train==-1] = 0\n",
    "    X_test = data.loc[test_index]['Text']\n",
    "    y_test = data.loc[test_index]['Sentiment']\n",
    "    y_test[y_test==-1] = 0\n",
    "    # Calculate TF-IDF for Naive Bayes classification\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "    # Get TF-IDF for Train and Test data\n",
    "    X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "    X_test_tfidf = tf_idf.transform(X_test)\n",
    " \n",
    "    best_model = MultinomialNB(alpha=0.6)\n",
    "    best_model.fit(X_train_tfidf, y_train)\n",
    "    probs = best_model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mses[0,i] = round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2)\n",
    "    i=i+1\n",
    "print('Naive-Bayes Accuracy (K-Fold):', mses.mean(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "train_pct = .8\n",
    "np.random.seed(10)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT processing function\n",
    "def prep(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in data:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=i, \n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\", \n",
    "                return_attention_mask=True )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  48\n"
     ]
    }
   ],
   "source": [
    "#Setting up for BERT\n",
    "encoded = [tokenizer.encode(i, add_special_tokens=True) for i in data['Text'].values]\n",
    "MAX_LEN = max([len(i) for i in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "X_train_inputs, X_train_masks = prep(X_train)\n",
    "X_test_inputs, X_test_masks = prep(X_test)\n",
    "\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        input_layer = 768\n",
    "        hidden_layer = 40\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for i in self.bert.parameters():\n",
    "                i.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        layer = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(layer)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.51820  |  Test Loss: 0.45014  |  Test Accuracy: 80.22\n",
      "Epoch: 2  |  Train Loss: 0.32035  |  Test Loss: 0.42441  |  Test Accuracy: 80.49\n",
      "Epoch: 3  |  Train Loss: 0.16949  |  Test Loss: 0.56377  |  Test Accuracy: 83.00\n",
      "Epoch: 4  |  Train Loss: 0.07111  |  Test Loss: 0.77743  |  Test Accuracy: 82.74\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(freeze=False)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "lr = 8e-5\n",
    "eps = 1e-8\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data; NOTICE: this accuracy function has to align with previous models\n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13793\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for negative sentiment:  76.22377622377621 %\n",
      "Accuracy for positive sentiment:  86.43835616438355 %\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(freeze=False)\n",
    "model.load_state_dict(torch.load('stock_sentiment_model.pt'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_inputs, batch_masks, batch_labels = batch\n",
    "\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "        \n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "negatives = np.where(y_test==0)[0]\n",
    "TNs = np.where( (y_test==0) & (y_test==predictions) )[0]\n",
    "print('Accuracy for negative sentiment: ',(len(TNs)/len(negatives))*100,'%')\n",
    "\n",
    "positives = np.where(y_test==1)[0]\n",
    "TPs = np.where( (y_test==1) & (y_test==predictions) )[0]\n",
    "print('Accuracy for positive sentiment: ',(len(TPs)/len(positives))*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model perform the best, we choose BERT for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
