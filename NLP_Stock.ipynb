{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\13793\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/13793/Desktop/aas/stock/twitter.csv', encoding='ISO-8859-1')\n",
    "data['Text'] = data['Text'].str.lower()\n",
    "\n",
    "## FIX HYPERLINKS\n",
    "data['Text'] = data['Text'].replace(r'https?:\\/\\/.*[\\r\\n]*', ' ',regex=True)\n",
    "data['Text'] = data['Text'].replace(r'www.*[\\r\\n]*', ' ',regex=True)\n",
    "data['Text'] = data['Text'].str.replace('https', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX INDIVIDUAL SYMBOLS \n",
    "data['Text'] = data['Text'].str.replace(': ', ' ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(', ', ' ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('. ', ' ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('[;\\n~]', ' ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].str.replace(\"[]'â€¦*™|]\", '', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('[[()!?\"]', '', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('_', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('w/', ' with ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('f/', ' for ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX EMOJIS\n",
    "data['Text'] = data['Text'].str.replace(':)', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(':-)', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(':(', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(':-(', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('0_o', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(';)', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('=^.^=', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX % SYMBOL\n",
    "data['Text'] = data['Text'].str.replace('%', ' percent ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX & SYMBOL\n",
    "data['Text'] = data['Text'].str.replace(' & ', ' and ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('&amp', ' and ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('&gt', ' greater than ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('cup&handle', 'cup and handle', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('c&h', 'cup and handle', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('head&shoulders', 'head and shoulders', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('h&s', 'head and shoulders', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('point&figure', 'point and figure', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('p&f', 'point and figure', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('s&p', 'SP500', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('q&a', 'question and answer', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('&', ' and ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX USER TAGS AND HASTAGS\n",
    "data['Text'] = data['Text'].str.replace('@[a-z0-9]+', '', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('#[a-z0-9]+', '', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('@', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('#', '', regex=False)\n",
    "\t   \n",
    "\t\t\n",
    "\t## FIX EMBEDDED COMMAS AND PERIODS    \n",
    "data['Text'] = data['Text'].replace(r'([a-z]),([a-z])', r'\\1 \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]),([0-9])', r'\\1\\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])[+]+', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(',', '', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('u.s.', ' us ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('\\.{2,}', ' ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([a-z])\\.([a-z])', r'\\1 \\2', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('pdating', 'updating', regex=False) \n",
    "data['Text'] = data['Text'].replace(r'([a-z])\\.', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'\\.([a-z])', r' \\1', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' . ', ' ', regex=False)\n",
    "\t\t\n",
    "\n",
    "\t## FIX + SYMBOL\n",
    "data['Text'] = data['Text'].replace(r'[+]([0-9])', r'positive \\1', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('c+h', 'cup and handle', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('h+s', 'head and shoulders', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('cup+handle', 'cup and handle', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' + ', ' and ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('+ ', ' ', regex=False)\n",
    "data['Text'] = data['Text'].replace(r'([a-z])[+]([a-z])', r'\\1 and \\2', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('+', '', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t## FIX - SYMBOL\n",
    "data['Text'] = data['Text'].replace(r'([a-z])[-]+([a-z])', r'\\1 \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([a-z]) - ([a-z])', r'\\1 to \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]) -([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r' [-]([0-9])', r' negative \\1', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])-([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]) - ([0-9\\.])', r'\\1 to \\2', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9a-z])-([0-9a-z])', r'\\1 \\2', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('[-]+[>]', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' [-]+ ', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('-', ' ', regex=False)\n",
    "\n",
    "\n",
    "\n",
    "\t## FIX $ SYMBOL\n",
    "data['Text'] = data['Text'].str.replace('[$][0-9\\.]', ' dollars ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('$', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX = SYMBOL\n",
    "data['Text'] = data['Text'].str.replace('=', ' equals ', regex=False)\n",
    "\n",
    "\t\t\n",
    "\t## FIX / SYMBOL\n",
    "data['Text'] = data['Text'].str.replace('b/c', ' because ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('b/out', ' break out ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('b/o', ' break out ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace('p/e', ' pe ratio ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' [/]+ ', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' 1/2 ', ' .5 ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 1/4 ', ' .25 ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 3/4 ', ' .75 ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 1/3 ', ' .3 ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 2/3 ', ' .6 ', regex=False)\n",
    "\n",
    "data['Text'] = data['Text'].str.replace('[/]{2,}', ' ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([a-z])/([a-z])', r'\\1 and \\2', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('[0-9]+/[0-9]+/[0-9]+', '', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]{3,})/([0-9\\.]{2,})', r'\\1 to \\2', regex=True)\t\n",
    "data['Text'] = data['Text'].replace(r'([0-9]{2,})/([0-9\\.]{3,})', r'\\1 to \\2', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('[a-z0-9]+/[a-z0-9]+', ' ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].str.replace('/', '', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX < > SYMBOLS\n",
    "data['Text'] = data['Text'].str.replace('[<]+ ', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('<', ' less than ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' [>]+', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('>', ' greater than ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX : SYMBOL\n",
    "data['Text'] = data['Text'].str.replace('[0-9]+:[0-9]+am', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('[0-9]+:[0-9]', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(':', ' ', regex=False)\n",
    "\n",
    "\n",
    "\t## FIX UNITS\n",
    "data['Text'] = data['Text'].str.replace('user ', ' ', regex=False)\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9]+)dma', r'\\1 displaced moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'dma([0-9]+)', r'\\1 displaced moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]+)sma', r'\\1 simple moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'sma([0-9]+)', r'\\1 simple moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]+)ema', r'\\1 expontential moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'ema([0-9]+)', r'\\1 expontential moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9]+)ma', r'\\1 moving average ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'ma([0-9]+)', r'\\1 moving average ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9])mos', r'\\1 months ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])minute', r'\\1 minute ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])minutes', r'\\1 minutes ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])min', r'\\1 minute ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])mins', r'\\1 minutes ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])day', r'\\1 day ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])days', r'\\1 days ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])wk', r'\\1 week ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' wk ', ' week ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' wknd ', ' weekend ', regex=False)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])wks', r'\\1 weeks ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])hours', r'\\1 hours ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])hour', r'\\1 hour ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])yr', r'\\1 year ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])yrs', r'\\1 years ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' yr', ' year ', regex=False)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])am', r'\\1 am ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])pm', r'\\1 pm ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9])est', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])ish', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9 ])pts', r'\\1 points ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])x', r'\\1 times ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])th', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])rd', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])st', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])nd', r'\\1 ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].str.replace('mrkt', 'market', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' vol ', ' volume ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' ptrend', ' positive trend ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' ppl', ' people ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' pts', ' points ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' pt', ' point ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' l(ol){1,}', ' laugh ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('imho', ' in my opinion ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace('prev ', 'previous ', regex=True)\n",
    "\n",
    "\n",
    "data['Text'] = data['Text'].str.replace(' 1q', ' first quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 2q', ' second quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 3q', ' third quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 4q', ' fourth quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' q1', ' first quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' q2', ' second quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' q3', ' third quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' q4', ' fourth quarter ', regex=False)\n",
    "data['Text'] = data['Text'].str.replace(' 10q ', ' form 10 ', regex=False)\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9])million', r'\\1 million ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])mil', r'\\1 million ', regex=True)\n",
    "data['Text'] = data['Text'].str.replace(' mil ', ' million ', regex=False)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])billion', r'\\1 billion ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])cents', r'\\1 cents ', regex=True)\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9])3d', r'\\1 3 dimensional ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])gb', r'\\1 3 gigabytes ', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "data['Text'] = data['Text'].replace(r'([0-9])c', r'\\1 calls ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])y', r'\\1 year ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])p', r'\\1 puts ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])d', r'\\1 days ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])h', r'\\1 hour ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])s', r'\\1 ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])k1', r'\\1 thousand ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])k', r'\\1 thousand ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])m', r'\\1 million ', regex=True)\n",
    "data['Text'] = data['Text'].replace(r'([0-9])b', r'\\1 billion ', regex=True)\n",
    "\n",
    "\t\t\n",
    "data['Text'] = data['Text'].replace(r'([0-9])([a-z])', r'\\1 \\2', regex=True)\n",
    "\n",
    "\t## FIX EXTRA SPACES AND ENDING PUNCTUATION\n",
    "data['Text'] = data['Text'].str.replace(' +', ' ', regex=True)\n",
    "data['Text'] = data['Text'].str.strip(' .!?,)(:-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification the positive/negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear up meaningless words for NB\n",
    "Stop = set([s.replace(\"'\", '') for s in stopwords.words('english') if s not in ['not', 'up', 'down', 'above', 'below', 'under', 'against','no','shouldnt']])\n",
    "data['Text'] = data['Text'].apply(lambda s: \" \".join([word for word in s.split() if word not in Stop]))\n",
    "data['Text'] = data['Text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Accuracy: 66.14 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SentimentIntensityAnalyzer()\n",
    "#baseline\n",
    "data['V_score'] = data['Text'].apply(lambda score: model.polarity_scores(score)['compound'])\n",
    "data['V_prediction'] = data['V_score'].apply(lambda score: 1 if score>=0 else -1)\n",
    "print('Vader Accuracy:', round((len(data[data['Sentiment']==data['V_prediction']])/len(data)) *100, 2), '%', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  0.6   |  Best Score:  84.35\n",
      "Naive-Bayes Accuracy: 71.7 %\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayesian Train/Test\n",
    "train_pct = .8\n",
    "np.random.seed(100)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "\n",
    "# Calculate TF-IDF for Naive Bayes classification\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "# Get TF-IDF for Train and Test data\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "X_test_tfidf = tf_idf.transform(X_test)\n",
    "\n",
    "\n",
    "# Define function to determine accuracy of model\n",
    "def get_auc_CV(model):\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    auc = cross_val_score(model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()\n",
    "\n",
    "# Find best performing model\n",
    "alphas = np.arange(0.4,1.5,0.1)\n",
    "models = [MultinomialNB(alpha=i) for i in alphas]\n",
    "accs = []\n",
    "for model in models:\n",
    "    accs.append(get_auc_CV(model))\n",
    "accs = np.array(accs)\n",
    "best_alpha = round(alphas[accs.argmax()], 1)\n",
    "\n",
    "# Print best alpha value and accuracy\n",
    "print('Best alpha: ', best_alpha, '  |  Best Score: ', round(accs.max()*100, 2))\n",
    "\n",
    "# Retrain best performing model\n",
    "best_model = MultinomialNB(alpha=best_alpha)\n",
    "best_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict test data with best model\n",
    "probs = best_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Print accuracy of best performing model on tweet sentiment analysis \n",
    "print('Naive-Bayes Accuracy:', round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes Accuracy (K-Fold): 71.804 %\n"
     ]
    }
   ],
   "source": [
    "#K-Fold\n",
    "kfold = KFold(n_splits=5,\n",
    "                 shuffle=True,\n",
    "                 random_state=132)\n",
    "mses = np.zeros((1, 5))\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(data):\n",
    "    X_train = data.loc[train_index]['Text']\n",
    "    y_train = data.loc[train_index]['Sentiment']\n",
    "    y_train[y_train==-1] = 0\n",
    "    X_test = data.loc[test_index]['Text']\n",
    "    y_test = data.loc[test_index]['Sentiment']\n",
    "    y_test[y_test==-1] = 0\n",
    "    # Calculate TF-IDF for Naive Bayes classification\n",
    "    tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "\n",
    "    # Get TF-IDF for Train and Test data\n",
    "    X_train_tfidf = tf_idf.fit_transform(X_train)\n",
    "    X_test_tfidf = tf_idf.transform(X_test)\n",
    " \n",
    "    best_model = MultinomialNB(alpha=0.6)\n",
    "    best_model.fit(X_train_tfidf, y_train)\n",
    "    probs = best_model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mses[0,i] = round(len(np.where(y_test == probs.argmax(axis=1))[0])/len(probs) * 100, 2)\n",
    "    i=i+1\n",
    "print('Naive-Bayes Accuracy (K-Fold):', mses.mean(), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "train_pct = .8\n",
    "np.random.seed(10)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT processing function\n",
    "def prep(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in data:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=i, \n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\", \n",
    "                return_attention_mask=True )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  53\n"
     ]
    }
   ],
   "source": [
    "#Setting up for BERT\n",
    "encoded = [tokenizer.encode(i, add_special_tokens=True) for i in data['Text'].values]\n",
    "MAX_LEN = max([len(i) for i in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "X_train_inputs, X_train_masks = prep(X_train)\n",
    "X_test_inputs, X_test_masks = prep(X_test)\n",
    "\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8705ba78a2748fa85fc7e1df677c6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13793\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.53139  |  Test Loss: 0.42354  |  Test Accuracy: 79.38\n",
      "Epoch: 2  |  Train Loss: 0.32014  |  Test Loss: 0.39786  |  Test Accuracy: 82.08\n",
      "Epoch: 3  |  Train Loss: 0.15164  |  Test Loss: 0.62618  |  Test Accuracy: 81.63\n",
      "Epoch: 4  |  Train Loss: 0.06619  |  Test Loss: 0.75469  |  Test Accuracy: 83.13\n"
     ]
    }
   ],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for i in self.bert.parameters():\n",
    "                i.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        layer = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(layer)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "model = BertClassifier(freeze=False)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "lr = 5e-5\n",
    "eps = 1e-8\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data; NOTICE: this accuracy function has to align with previous models\n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
